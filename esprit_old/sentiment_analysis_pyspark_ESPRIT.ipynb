{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a pyspark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ESPRIT Sentiment Analysis Case\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparknlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-30ad802d5e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparknlp'"
     ]
    }
   ],
   "source": [
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    0|\"Reference Yes, L...|\n",
      "|    0|NO!!!!!!I will gi...|\n",
      "|    0|Neat Features/Unc...|\n",
      "|    1|\"Progressive-Unde...|\n",
      "|    0|\"The theif who st...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_train = spark.read.options(delimiter=';').csv('train data product reviews.csv', inferSchema=True, header=True)\n",
    "comments_train.show(truncate=True, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51979, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_train.count(), comments_train.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'label' column we have 0's and 1's only. Let's rearrange this data frame as *df_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|\"Reference Yes, L...|    0|\n",
      "|NO!!!!!!I will gi...|    0|\n",
      "|Neat Features/Unc...|    0|\n",
      "|\"Progressive-Unde...|    1|\n",
      "|\"The theif who st...|    0|\n",
      "|This movie was te...|    0|\n",
      "|I love wood: I ha...|    1|\n",
      "|Lets me use my ow...|    1|\n",
      "|Good for study pu...|    0|\n",
      "|Great for Beadwea...|    1|\n",
      "|gifts for people:...|    0|\n",
      "|Very good read!: ...|    1|\n",
      "|Truly Wonderful: ...|    1|\n",
      "|\"Africa de mi cor...|    1|\n",
      "|good, but i need ...|    1|\n",
      "|Turntable with Au...|    0|\n",
      "|\"Very poorly writ...|    0|\n",
      "|Tired, immature, ...|    0|\n",
      "|Husband Gerald Br...|    0|\n",
      "|more of a clash t...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = comments_train.select('text', 'label')\n",
    "df_train =df_train.withColumn('label', df_train.label.cast(IntegerType()))\n",
    "df_train.show(truncate=True, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to maintain a *df_test* similar to *df_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          label,text|\n",
      "+--------------------+\n",
      "|0,Not worth the m...|\n",
      "|\"0,\"\"I changed my...|\n",
      "|\"0,\"\"How quickly ...|\n",
      "|0,DOA Did Not Pow...|\n",
      "|\"0,\"\"support: I o...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_test = spark.read.options(delimiter=';').csv('test data product reviews.csv', inferSchema=True, header=True)\n",
    "comments_test.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use *regex* to describe patters to obtain a clean data frame with columns text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|,Not worth the mo...|    0|\n",
      "|,\"\"I changed my m...|    0|\n",
      "|,\"\"How quickly we...|    0|\n",
      "|,DOA Did Not Powe...|    0|\n",
      "|,\"\"support: I ord...|    0|\n",
      "|,\"\"Rewriting this...|    1|\n",
      "|,\"\"Canon CLI-8 4-...|    1|\n",
      "|,needs parts: My ...|    0|\n",
      "|,\"\"Awesome: Does ...|    1|\n",
      "|,Yeah for Dairy F...|    1|\n",
      "|,\"\"Good book if y...|    0|\n",
      "|,\"\"Good way to ke...|    1|\n",
      "|,\"\"The Best Red S...|    1|\n",
      "|,\"\"Piece of Crap:...|    0|\n",
      "|,\"\"SO EASY!!!!: T...|    1|\n",
      "|,Very Useful Info...|    1|\n",
      "|,\"\"great product!...|    1|\n",
      "|,\"\"Breathtakingly...|    0|\n",
      "|,\"\"I am thankful ...|    1|\n",
      "|,Very good: Great...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_pattern = r'\"*([01])(.+)'\n",
    "comments_test = comments_test.withColumn('text', regexp_extract(col('label,text'), regex_pattern, 2))\\\n",
    "                 .withColumn('label', regexp_extract(col('label,text'), regex_pattern, 1))\n",
    "df_test = comments_test.select('text', 'label')\n",
    "df_test =df_test.withColumn('label', df_test.label.cast(IntegerType()))\n",
    "df_test.show(truncate=True, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11703, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count(), df_test.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both *df_train* and *df_test* in our targetted composition, we can progress with the **sentiment analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing (Training Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|     sentiment_words|\n",
      "+--------------------+-----+--------------------+\n",
      "|\"Reference Yes, L...|    0|[\"reference, yes,...|\n",
      "|NO!!!!!!I will gi...|    0|[no!!!!!!i, will,...|\n",
      "|Neat Features/Unc...|    0|[neat, features/u...|\n",
      "|\"Progressive-Unde...|    1|[\"progressive-und...|\n",
      "|\"The theif who st...|    0|[\"the, theif, who...|\n",
      "|This movie was te...|    0|[this, movie, was...|\n",
      "|I love wood: I ha...|    1|[i, love, wood:, ...|\n",
      "|Lets me use my ow...|    1|[lets, me, use, m...|\n",
      "|Good for study pu...|    0|[good, for, study...|\n",
      "|Great for Beadwea...|    1|[great, for, bead...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='sentiment_words')\n",
    "tokenized_train = tokenizer.transform(df_train)\n",
    "tokenized_train.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|                text|label|     sentiment_words|      decisive_words|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|\"Reference Yes, L...|    0|[\"reference, yes,...|[\"reference, yes,...|\n",
      "|NO!!!!!!I will gi...|    0|[no!!!!!!i, will,...|[no!!!!!!i, give,...|\n",
      "|Neat Features/Unc...|    0|[neat, features/u...|[neat, features/u...|\n",
      "|\"Progressive-Unde...|    1|[\"progressive-und...|[\"progressive-und...|\n",
      "|\"The theif who st...|    0|[\"the, theif, who...|[\"the, theif, sto...|\n",
      "|This movie was te...|    0|[this, movie, was...|[movie, terrible!...|\n",
      "|I love wood: I ha...|    1|[i, love, wood:, ...|[love, wood:, in-...|\n",
      "|Lets me use my ow...|    1|[lets, me, use, m...|[lets, use, coffe...|\n",
      "|Good for study pu...|    0|[good, for, study...|[good, study, pur...|\n",
      "|Great for Beadwea...|    1|[great, for, bead...|[great, beadweavi...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='decisive_words')\n",
    "swr_free_train = swr.transform(tokenized_train)\n",
    "swr_free_train.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|      decisive_words|        raw_features|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|[\"reference, yes,...|(262144,[11946,14...|    0|\n",
      "|[no!!!!!!i, give,...|(262144,[6102,137...|    0|\n",
      "|[neat, features/u...|(262144,[2306,278...|    0|\n",
      "|[\"progressive-und...|(262144,[1148,576...|    1|\n",
      "|[\"the, theif, sto...|(262144,[1546,188...|    0|\n",
      "|[movie, terrible!...|(262144,[1696,424...|    0|\n",
      "|[love, wood:, in-...|(262144,[12524,41...|    1|\n",
      "|[lets, use, coffe...|(262144,[1354,538...|    1|\n",
      "|[good, study, pur...|(262144,[4578,757...|    0|\n",
      "|[great, beadweavi...|(262144,[2574,649...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"raw_features\")\n",
    "numeric_train = hashingTF.transform(swr_free_train).select('decisive_words','raw_features', 'label')\n",
    "numeric_train.show(truncate=True, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lr is trained\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(labelCol='label', featuresCol='raw_features', maxIter = 5, regParam=.001)\n",
    "model_lr = logreg.fit(numeric_train)\n",
    "print('model_lr is trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|      decisive_words|        raw_features|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|[,not, worth, mon...|(262144,[7777,132...|    0|\n",
      "|[,\"\"i, changed, m...|(262144,[1578,230...|    0|\n",
      "|[,\"\"how, quickly,...|(262144,[2448,861...|    0|\n",
      "|[,doa, power, box...|(262144,[35590,38...|    0|\n",
      "|[,\"\"support:, ord...|(262144,[4907,997...|    0|\n",
      "|[,\"\"rewriting, ma...|(262144,[2325,748...|    1|\n",
      "|[,\"\"canon, cli-8,...|(262144,[77073,84...|    1|\n",
      "|[,needs, parts:, ...|(262144,[1546,538...|    0|\n",
      "|[,\"\"awesome:, sup...|(262144,[11422,44...|    1|\n",
      "|[,yeah, dairy, fr...|(262144,[5729,762...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_test = tokenizer.transform(df_test)\n",
    "swr_free_test = swr.transform(tokenized_test)\n",
    "numeric_test = hashingTF.transform(swr_free_test).select('decisive_words','raw_features', 'label')\n",
    "numeric_test.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|      decisive_words|prediction|label|\n",
      "+--------------------+----------+-----+\n",
      "|[,not, worth, mon...|         0|    0|\n",
      "|[,\"\"i, changed, m...|         0|    0|\n",
      "|[,\"\"how, quickly,...|         0|    0|\n",
      "|[,doa, power, box...|         0|    0|\n",
      "|[,\"\"support:, ord...|         0|    0|\n",
      "|[,\"\"rewriting, ma...|         1|    1|\n",
      "|[,\"\"canon, cli-8,...|         1|    1|\n",
      "|[,needs, parts:, ...|         0|    0|\n",
      "|[,\"\"awesome:, sup...|         1|    1|\n",
      "|[,yeah, dairy, fr...|         0|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_logreg = model_lr.transform(numeric_test)\n",
    "predicted_logreg_df = predict_logreg.select(\n",
    "    \"decisive_words\", \"prediction\", \"label\")\n",
    "predicted_logreg_df = predicted_logreg_df.withColumn('prediction', predicted_logreg_df.prediction.cast(IntegerType()))\n",
    "predicted_logreg_df.show(truncate = True, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(df,prediction,label):\n",
    "    \"\"\"\n",
    "    Generates a manual confusion matrix in a pyspark data frame, which is assembled according to the classification prediction. \n",
    "    df = Data Frame with prediction and label values\n",
    "    prediction = string, column name of the prediction values\n",
    "    label = string, column name of the label values\n",
    "    \"\"\"\n",
    "    correctly_predicted = df.filter(df.prediction == df.label).count()\n",
    "    false_positive = df.filter((df.prediction == 1) & (df.label == 0)).count()\n",
    "    false_negative = df.filter((df.prediction == 0) & (df.label == 1)).count()\n",
    "    true_positive = df.filter((df.prediction == 1) & (df.label == 1)).count()\n",
    "    true_negative = df.filter((df.prediction == 0) & (df.label == 0)).count()\n",
    "    \n",
    "    accuracy = correctly_predicted/df.count()\n",
    "    precision = true_positive/(true_positive + false_positive)\n",
    "    recall = true_positive/(true_positive + false_negative)\n",
    "    f1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "\n",
    "    \n",
    "    print(f'Correctly Predicted (True Positive): {correctly_predicted} which is %{correctly_predicted/df.count()}')\n",
    "    print(f'Type-I Error (False Positive): {false_positive} which is %{false_positive/df.count()}')\n",
    "    print(f'Type-II Error (False Negative): {false_negative} which is %{false_negative/df.count()}')\n",
    "    print(f'Accuracy: %{accuracy}')\n",
    "    print(f'Precision: %{precision}')\n",
    "    print(f'Sensitivity(Recall): %{recall}')\n",
    "    print(f'F1 Score: %{f1_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Predicted (True Positive): 9056 which is %0.7738186789712039\n",
      "Type-I Error (False Positive): 1709 which is %0.14603093223959668\n",
      "Type-II Error (False Negative): 938 which is %0.08015038878919935\n",
      "Accuracy: %0.7738186789712039\n",
      "Precision: %0.7406676783004552\n",
      "Sensitivity(Recall): %0.8388039181990032\n",
      "F1 Score: %0.7866870819566444\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(predicted_logreg_df, 'prediction','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"raw_features\")\n",
    "model_nb = nb.fit(numeric_train)\n",
    "nb_predictions = model_nb.transform(numeric_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes is = 0.811416\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Predicted (True Positive): 9496 which is %0.8114158762710416\n",
      "Type-I Error (False Positive): 1261 which is %0.10775014953430745\n",
      "Type-II Error (False Negative): 946 which is %0.08083397419465095\n",
      "Accuracy: %0.8114158762710416\n",
      "Precision: %0.7944245190740137\n",
      "Sensitivity(Recall): %0.8374291115311909\n",
      "F1 Score: %0.8153601606291307\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(nb_predictions, 'prediction','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Fitting the model\n",
    "dt = DecisionTreeClassifier(featuresCol = 'raw_features', labelCol = 'label', maxDepth = 3)\n",
    "model_dt = dt.fit(numeric_train)\n",
    "dt_predictions = model_dt.transform(numeric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Trees is = 0.554473\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Predicted (True Positive): 6489 which is %0.5544732119969239\n",
      "Type-I Error (False Positive): 5088 which is %0.43476031786721353\n",
      "Type-II Error (False Negative): 126 which is %0.0107664701358626\n",
      "Accuracy: %0.5544732119969239\n",
      "Precision: %0.5280586216491977\n",
      "Sensitivity(Recall): %0.9783467949819556\n",
      "F1 Score: %0.6859036144578313\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(dt_predictions, 'prediction','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----------------+--------------------+----------+\n",
      "|      decisive_words|        raw_features|label|    rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+----------+\n",
      "|[,not, worth, mon...|(262144,[7777,132...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"i, changed, m...|(262144,[1578,230...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"how, quickly,...|(262144,[2448,861...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,doa, power, box...|(262144,[35590,38...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"support:, ord...|(262144,[4907,997...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"rewriting, ma...|(262144,[2325,748...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"canon, cli-8,...|(262144,[77073,84...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,needs, parts:, ...|(262144,[1546,538...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"awesome:, sup...|(262144,[11422,44...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,yeah, dairy, fr...|(262144,[5729,762...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"good, book, 3...|(262144,[12898,17...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"good, way, ke...|(262144,[3856,553...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"the, best, re...|(262144,[5675,105...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,\"\"piece, crap:,...|(262144,[7625,246...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"so, easy!!!!:...|(262144,[10049,16...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,very, useful, i...|(262144,[13231,29...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"great, produc...|(262144,[2437,576...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"breathtakingl...|(262144,[1164,421...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"i, thankful, ...|(262144,[3091,339...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,very, good:, gr...|(262144,[8750,141...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,good, colorful,...|(262144,[12020,21...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"excited, read...|(262144,[12898,13...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,a, surprisingly...|(262144,[14877,15...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,not, worth, mon...|(262144,[2306,538...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"gives, metalc...|(262144,[3870,657...|    0|    [879.0,305.0]|[0.74239864864864...|       0.0|\n",
      "|[,good, informati...|(262144,[15391,31...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,disapointing, a...|(262144,[6946,113...|    0|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"great, produc...|(262144,[1578,538...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "|[,\"\"excellent, lo...|(262144,[25927,30...|    1|[11457.0,26747.0]|[0.29989006386765...|       1.0|\n",
      "|[,\"\"excellent, wo...|(262144,[370,8316...|    1| [1111.0,10147.0]|[0.09868537928584...|       1.0|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_predictions.show(truncate=True, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to tokens, removing punctuation, removing stop words, performing stemming and lemmatization using SPARK NLP Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.1.1\n",
      "Apache Spark version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.config(\"spark.jars\", \"hdfs://somepath/sparknlp.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## I. Spark NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d343195fc7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocumentAssembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentenceDetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sparknlp/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"com.johnsnowlabs.nlp.DocumentAssembler\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanupMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'disabled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"])\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"removed_stopwords\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               sentenceDetector,\n",
    "                               tokenizer,\n",
    "                               normalizer,\n",
    "                               stopwords_cleaner,\n",
    "                               stemmer,\n",
    "                               lemmatizer,\n",
    "                               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparknlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-438c49067f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparknlp'"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
