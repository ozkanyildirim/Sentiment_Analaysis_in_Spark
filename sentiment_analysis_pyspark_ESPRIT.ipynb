{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a pyspark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+--------------------+\n|label|                text|\n+-----+--------------------+\n|    0|\"Reference Yes, L...|\n|    0|NO!!!!!!I will gi...|\n|    0|Neat Features/Unc...|\n|    1|\"Progressive-Unde...|\n|    0|\"The theif who st...|\n+-----+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "comments_train = spark.read.options(delimiter=';').csv('train data product reviews.csv', inferSchema=True, header=True)\n",
    "comments_train.show(truncate=True, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(51979, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "comments_train.count(), comments_train.select('label').distinct().count()"
   ]
  },
  {
   "source": [
    "In 'label' column we have 0's and 1's only. Let's rearrange this data frame as *df_train*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|                text|label|\n+--------------------+-----+\n|\"Reference Yes, L...|    0|\n|NO!!!!!!I will gi...|    0|\n|Neat Features/Unc...|    0|\n|\"Progressive-Unde...|    1|\n|\"The theif who st...|    0|\n|This movie was te...|    0|\n|I love wood: I ha...|    1|\n|Lets me use my ow...|    1|\n|Good for study pu...|    0|\n|Great for Beadwea...|    1|\n|gifts for people:...|    0|\n|Very good read!: ...|    1|\n|Truly Wonderful: ...|    1|\n|\"Africa de mi cor...|    1|\n|good, but i need ...|    1|\n|Turntable with Au...|    0|\n|\"Very poorly writ...|    0|\n|Tired, immature, ...|    0|\n|Husband Gerald Br...|    0|\n|more of a clash t...|    0|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_train = comments_train.select('text', 'label')\n",
    "df_train =df_train.withColumn('label', df_train.label.cast(IntegerType()))\n",
    "df_train.show(truncate=True, n=20)"
   ]
  },
  {
   "source": [
    "Now we are going to maintain a *df_test* similar to *df_train*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n|          label,text|\n+--------------------+\n|0,Not worth the m...|\n|\"0,\"\"I changed my...|\n|\"0,\"\"How quickly ...|\n|0,DOA Did Not Pow...|\n|\"0,\"\"support: I o...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "comments_test = spark.read.options(delimiter=';').csv('test data product reviews.csv', inferSchema=True, header=True)\n",
    "comments_test.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11703"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "comments_test.count()"
   ]
  },
  {
   "source": [
    "We are going to use *regex* to describe patters to obtain a clean data frame with columns text and label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|                text|label|\n+--------------------+-----+\n|,Not worth the mo...|    0|\n|,\"\"I changed my m...|    0|\n|,\"\"How quickly we...|    0|\n|,DOA Did Not Powe...|    0|\n|,\"\"support: I ord...|    0|\n|,\"\"Rewriting this...|    1|\n|,\"\"Canon CLI-8 4-...|    1|\n|,needs parts: My ...|    0|\n|,\"\"Awesome: Does ...|    1|\n|,Yeah for Dairy F...|    1|\n|,\"\"Good book if y...|    0|\n|,\"\"Good way to ke...|    1|\n|,\"\"The Best Red S...|    1|\n|,\"\"Piece of Crap:...|    0|\n|,\"\"SO EASY!!!!: T...|    1|\n|,Very Useful Info...|    1|\n|,\"\"great product!...|    1|\n|,\"\"Breathtakingly...|    0|\n|,\"\"I am thankful ...|    1|\n|,Very good: Great...|    1|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "regex_pattern = r'\"*([01])(.+)'\n",
    "comments_test = comments_test.withColumn('text', regexp_extract(col('label,text'), regex_pattern, 2))\\\n",
    "                 .withColumn('label', regexp_extract(col('label,text'), regex_pattern, 1))\n",
    "df_test = comments_test.select('text', 'label')\n",
    "df_test =df_test.withColumn('label', df_test.label.cast(IntegerType()))\n",
    "df_test.show(truncate=True, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11703, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "df_test.count(), df_test.select('label').distinct().count()"
   ]
  },
  {
   "source": [
    "Now that we have both *df_train* and *df_test* in our targetted composition, we can progress with the **sentiment analysis**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Data Preparation (Training Data)**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Tokenizer**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+--------------------+\n|                text|label|     sentiment_words|\n+--------------------+-----+--------------------+\n|\"Reference Yes, L...|    0|[\"reference, yes,...|\n|NO!!!!!!I will gi...|    0|[no!!!!!!i, will,...|\n|Neat Features/Unc...|    0|[neat, features/u...|\n|\"Progressive-Unde...|    1|[\"progressive-und...|\n|\"The theif who st...|    0|[\"the, theif, who...|\n|This movie was te...|    0|[this, movie, was...|\n|I love wood: I ha...|    1|[i, love, wood:, ...|\n|Lets me use my ow...|    1|[lets, me, use, m...|\n|Good for study pu...|    0|[good, for, study...|\n|Great for Beadwea...|    1|[great, for, bead...|\n+--------------------+-----+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='sentiment_words')\n",
    "tokenized_train = tokenizer.transform(df_train)\n",
    "tokenized_train.show(truncate=True, n=10)"
   ]
  },
  {
   "source": [
    "**Removing Stop Words**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n|                text|label|     sentiment_words|      decisive_words|\n+--------------------+-----+--------------------+--------------------+\n|\"Reference Yes, L...|    0|[\"reference, yes,...|[\"reference, yes,...|\n|NO!!!!!!I will gi...|    0|[no!!!!!!i, will,...|[no!!!!!!i, give,...|\n|Neat Features/Unc...|    0|[neat, features/u...|[neat, features/u...|\n|\"Progressive-Unde...|    1|[\"progressive-und...|[\"progressive-und...|\n|\"The theif who st...|    0|[\"the, theif, who...|[\"the, theif, sto...|\n|This movie was te...|    0|[this, movie, was...|[movie, terrible!...|\n|I love wood: I ha...|    1|[i, love, wood:, ...|[love, wood:, in-...|\n|Lets me use my ow...|    1|[lets, me, use, m...|[lets, use, coffe...|\n|Good for study pu...|    0|[good, for, study...|[good, study, pur...|\n|Great for Beadwea...|    1|[great, for, bead...|[great, beadweavi...|\n+--------------------+-----+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='decisive_words')\n",
    "swr_free_train = swr.transform(tokenized_train)\n",
    "swr_free_train.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}